# ğŸ§  AutoML Debugger  
### LLM-Assisted Dataset Diagnostics for Machine Learning Engineers

> **Know whether your dataset is worth training on â€” before you waste compute.**

AutoML Debugger is an **industry-grade ML diagnostics tool** that automatically evaluates dataset quality, detects weak predictive signals, and explains results using **LLM-powered expert reasoning** â€” even when **no dataset is uploaded**.

This project is inspired by **real internal tools used by ML teams** to validate data *before* model deployment.

---

## â­ Why This Repo Deserves a Star

âœ”ï¸ Solves a **real ML engineering problem**  
âœ”ï¸ Handles **messy real-world datasets**  
âœ”ï¸ Uses **LLMs for reasoning, not gimmicks**  
âœ”ï¸ Robust fallback system (never breaks)  
âœ”ï¸ Clean, recruiter-friendly UI  
âœ”ï¸ Not another â€œtrain a modelâ€ project  

If you work with ML data â€” this tool is useful.

---

## ğŸš€ What Problem Does This Solve?

Most ML failures happen **because of bad data**, not bad models.

AutoML Debugger answers:
- â“ *Does my dataset have predictive signal?*
- â“ *Is this dataset production-ready?*
- â“ *Why is my model performing poorly?*
- â“ *What should I fix first?*

All **automatically**, with **clear explanations**.

---

## âœ¨ Key Features

### ğŸ“Š Quantitative ML Health Metrics
- Train / validation performance (RÂ², error trends)
- Automatic weak-signal detection
- Clear dataset diagnosis

### ğŸ§ª Robust Data Handling (Zero-Crash Design)
- Handles:
  - Missing values (NaNs)
  - Mixed numeric & categorical features
  - Invalid data types
- Uses sklearn Pipelines (production-safe)

### ğŸ” Fallback Dataset Support (Important)
- If **no dataset is uploaded**:
  - The app automatically loads a **stored fallback dataset**
  - Runs the full AutoML debugging pipeline
- The **Run AutoML Debugger** button always works

### ğŸ§  LLM-Based Expert Analysis
- Uses a real LLM (OpenAI / Gemini-ready)
- Outputs **clear bullet-point explanations**, such as:
  - What the model learned
  - Why performance is weak or strong
  - Which dataset properties caused issues
- Written so **recruiters & non-ML stakeholders** understand

### ğŸ–¥ï¸ Clean Streamlit UI
- Dark-mode friendly interface
- Step-by-step flow
- No disruptive red error banners
- Clear separation of metrics & explanations

---

## ğŸ—ï¸ How It Works (System Overview)

1ï¸âƒ£ **Dataset Ingestion**  
- User uploads CSV **or**
- App automatically loads fallback dataset  

2ï¸âƒ£ **Preprocessing Pipeline**  
- Numeric â†’ imputation + scaling  
- Categorical â†’ safe encoding  
- Fully sklearn-pipeline based  

3ï¸âƒ£ **Baseline Model Training**  
- Lightweight, interpretable regression model  
- Designed for diagnostics (not leaderboard chasing)

4ï¸âƒ£ **Metric Evaluation**  
- Model performance computed
- Predictive signal strength assessed

5ï¸âƒ£ **LLM Explanation Layer**  
- Metrics passed to LLM
- LLM returns **structured bullet-point analysis**
- Explains *why* results look the way they do

---

## ğŸ§  Example LLM Output

- The model shows **low RÂ²**, indicating weak correlation between features and target.
- High noise and limited feature relevance reduce predictive power.
- Dataset likely requires:
  - Feature engineering
  - Target redefinition
  - Larger or cleaner data
- Current dataset is **not production-ready** without improvements.

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Streamlit** â€” UI
- **Pandas / NumPy** â€” Data handling
- **Scikit-Learn (locked version)** â€” ML pipelines
- **LLM API (OpenAI / Gemini compatible)** â€” Expert reasoning

---

## ğŸ“¦ Dependency Stability (No Version Chaos)

This project uses **locked versions** to guarantee reproducibility:

```txt
streamlit==1.31.0
pandas==2.1.4
numpy==1.26.4
scikit-learn==1.3.2
scipy==1.11.4
matplotlib==3.8.2


## Author
Nishant Diwate
